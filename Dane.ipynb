{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Dane.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8_frITM71T2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI9Z5r8MRE49"
      },
      "source": [
        "# pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vcOAJyNRNsm"
      },
      "source": [
        "# pip install -U textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxW-9xR5h3au"
      },
      "source": [
        "# pip install langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWn7beXENnAB"
      },
      "source": [
        "import re, string, gc\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import SyllableTokenizer\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "from scipy import sparse\n",
        "\n",
        "from textstat import sentence_count\n",
        "from langdetect import detect, detect_langs\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h11KBn3UOAOa"
      },
      "source": [
        "root_dir = \"gdrive/MyDrive/Uczelnia/Magisterka/Datasets\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmJPgFaR-v2Z"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5iQW5uk-uH7"
      },
      "source": [
        "def display_all(toPrint):\n",
        "    pd.options.display.max_colwidth = None\n",
        "    pd.options.display.max_rows = None\n",
        "    display(toPrint)\n",
        "    pd.reset_option(\"display.max_colwidth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KsXERFiNpbp"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20JWqzTFPVi0"
      },
      "source": [
        "def remove_numbers(s):\n",
        "    print(s)\n",
        "    return re.sub(r'[0-9][0-9.,-]*', '', s)\n",
        "\n",
        "def remove_stop_words(example_sent):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    word_tokens = word_tokenize(example_sent)\n",
        "    \n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    \n",
        "    filtered_sentence = []\n",
        "    \n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "            \n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "def pos_parts(text_words):\n",
        "    poses = dict(Counter(dict(pos_tag(text_words, tagset='universal')).values()))\n",
        "    all_values = sum(poses.values())\n",
        "\n",
        "    def get_pos_part(tag):\n",
        "        return poses.get(tag, 0)/all_values\n",
        "\n",
        "    return [get_pos_part('ADJ'), get_pos_part('ADP'), get_pos_part('ADV'), get_pos_part('CONJ'), get_pos_part('DET'), get_pos_part('NOUN'), get_pos_part('NUM'), get_pos_part('PRON'), get_pos_part('PRT'), get_pos_part('VERB')]\n",
        "\n",
        "\n",
        "def counts(text):\n",
        "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
        "    number_of_punctuation = count(text, string.punctuation)\n",
        "    number_of_capitals = sum(1 for c in text if c.isupper())\n",
        "    number_of_lowers = sum(1 for c in text if c.islower())\n",
        "    text_words = word_tokenize(remove_punctuation(text))\n",
        "\n",
        "    number_of_stopwords = len([w for w in text_words if w in stopwords.words('english')])\n",
        "    poss = pos_parts(text_words)\n",
        "\n",
        "    vectorizer = SyllableTokenizer()\n",
        "\n",
        "    number_of_sentences = sentence_count(text) * 1.0\n",
        "    number_of_words = len(text_words) * 1.0\n",
        "\n",
        "    if len(text_words) > 0:\n",
        "        number_of_syllables = np.concatenate([vectorizer.tokenize(word) for word in text_words]).size * 1.0\n",
        "    else:\n",
        "        number_of_syllables = 0\n",
        "\n",
        "    values = [number_of_sentences, number_of_words, number_of_syllables, number_of_punctuation, number_of_capitals, number_of_lowers, number_of_stopwords/number_of_words]\n",
        "    values.extend(poss)\n",
        "    return values\n",
        "\n",
        "def remove_punctuation(s):\n",
        "    return re.sub('[^\\w\\s]', \"\", s)\n",
        "\n",
        "def average_syllables_per_word(number_of_syllable, number_of_words):\n",
        "    return (number_of_syllable)/(number_of_words)\n",
        "\n",
        "def flesch_kincaid_grade(number_of_sentences, number_of_words, number_of_syllables):\n",
        "    return 0.39 * (number_of_words / number_of_sentences) + 11.8 * (number_of_syllables / number_of_words) - 15.59\n",
        "    \n",
        "def preproc(s):\n",
        "    return re.sub(r'[0-9][0-9.,-]*', ' NUMBERSPECIALTOKEN ', s).lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWAmHfAnNnAG"
      },
      "source": [
        "class DataSet:\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        print(texts)\n",
        "        d = np.array([counts(no_number_text) for no_number_text in [remove_numbers(text) for text in texts]])\n",
        "        print(d)\n",
        "        \n",
        "        self.counts = pd.DataFrame(d, columns=['# sentences', '# words', '# syllables', \"# punctuation\", \"# capitals\", \"# lowers\", \"% stopwords\", '% ADJ', '% ADP', '% ADV', '% CONJ', '% DET', '% NOUN', '% NUM', '% PRON', '% PRT', '% VERB'])\n",
        "        self.counts['avg syllables per word'] = average_syllables_per_word(self.counts['# syllables'], self.counts['# words'])\n",
        "        self.counts['F-K grade'] = flesch_kincaid_grade(self.counts['# sentences'], self.counts['# words'], self.counts['# syllables'])\n",
        "\n",
        "    def save_preprocessed(self, file_name):\n",
        "        initial = pd.DataFrame({\"text\": self.texts, \"label\": self.labels})\n",
        "        pd.concat([initial, self.counts], axis=1).to_csv(file_name, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlI0_jplNnAH"
      },
      "source": [
        "## ISOT Fake News Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpjw7pk62t2P"
      },
      "source": [
        "def remove_header(text):\n",
        "    if len(text.split()) > 2 and text.split()[1] == \"(Reuters)\":\n",
        "        return re.sub(r'^.*?-', '', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVY_HRelNnAK"
      },
      "source": [
        "fakes = pd.read_csv(root_dir + \"/Dataset/Fake.csv\",  index_col=False)\n",
        "fakes['label'] = np.full((fakes.shape[0]), \"fake\")\n",
        "\n",
        "truths = pd.read_csv(root_dir + \"/Dataset/True.csv\",  index_col=False)\n",
        "truths['label'] = np.full((truths.shape[0]), \"true\")\n",
        "\n",
        "truths.dropna(subset=['text'], inplace=True)\n",
        "fakes.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "truths['text'] = truths['text'].map(remove_header)\n",
        "truths.reset_index(drop=True)\n",
        "\n",
        "isot = pd.concat((fakes, truths), ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOGMLYRSeeUy"
      },
      "source": [
        "non_english = []\n",
        "for i, text in enumerate(isot['text']):\n",
        "    try: \n",
        "        lang = detect(text) \n",
        "        if lang != 'en':\n",
        "            non_english.append(i)\n",
        "    except:\n",
        "        non_english.append(i)\n",
        "\n",
        "isot.drop(index=non_english, inplace=True)\n",
        "# isot.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33V2FYggWwNr"
      },
      "source": [
        "isot_dataset = DataSet(isot['text'].values, isot['label'].values)\n",
        "isot_dataset.save_preprocessed(root_dir + \"/isot_dataset_preprocessed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmXUXsh8OXye"
      },
      "source": [
        "## Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iafUcmKSOZt4"
      },
      "source": [
        "kaggles = pd.read_csv(root_dir + \"/kaggle_dataset/kaggle_news_dataset.csv\", usecols=[1,2,4]).astype('U')\n",
        "kaggles = pd.read_csv(root_dir + \"/kaggle_dataset/kaggle_news_dataset.csv\").astype('U')\n",
        "\n",
        "non_english = []\n",
        "for i, text in enumerate(kaggles['content']):\n",
        "    try: \n",
        "        lang = detect(text) \n",
        "        if lang != 'en':\n",
        "            non_english.append(i)\n",
        "    except:\n",
        "        # print(text)\n",
        "        # print(\"-----\")\n",
        "        non_english.append(i)\n",
        "\n",
        "kaggles.drop(index=non_english, inplace=True)\n",
        "kaggles.dropna(inplace=True)\n",
        "\n",
        "kaggle_dataset = DataSet(kaggles['content'].values, kaggles['label'].values)\n",
        "kaggle_dataset.save_preprocessed(root_dir + \"/kaggle_dataset_preprocessed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}